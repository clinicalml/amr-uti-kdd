{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of Main Figures / Tables\n",
    "\n",
    "**Note**: See `README.md` in the main folder if you have not already, for instructions on how to generate the experiment results that this notebook uses.\n",
    "\n",
    "This notebook will replicate selected figures / tables in the main paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This flag can be set to use the same hyperparameters and thresholds \n",
    "# as in our published work.  Due to differences in features (described in README.md), there are still\n",
    "# some minor differences, but this will more closely replicate our published results\n",
    "USE_REP_HP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_palette('deep')\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rcParams['ps.useafm'] = True\n",
    "plt.rcParams['pdf.use14corefonts'] = True\n",
    "plt.rcParams['text.usetex'] = False\n",
    "\n",
    "import os\n",
    "\n",
    "REPO_PATH = os.environ['REPO_PATH']\n",
    "EXP_PATH = os.environ['EXP_RESULT_PATH']\n",
    "DIRECT_EXP_PATH = f\"{EXP_PATH}/direct/direct_learning_test\"\n",
    "\n",
    "if USE_REP_HP:\n",
    "    ER_EXP_PATH = f\"{EXP_PATH}/expected_reward/expected_reward_eval_test_rep/results\"\n",
    "    THRESH_EXP_PATH = f\"{EXP_PATH}/thresholding/thresholding_eval_test_rep/results\"\n",
    "else:\n",
    "    ER_EXP_PATH = f\"{EXP_PATH}/expected_reward/expected_reward_eval_test/results\"\n",
    "    THRESH_EXP_PATH = f\"{EXP_PATH}/thresholding/thresholding_eval_test/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify file paths here as needed\n",
    "direct_frontier = pd.read_csv(f\"{DIRECT_EXP_PATH}/frontier_test.csv\")\n",
    "exp_reward_frontier = pd.read_csv(f\"{ER_EXP_PATH}/frontier_test.csv\")\n",
    "thresholding_frontier = pd.read_csv(f\"{THRESH_EXP_PATH}/best_test_outcomes_by_max_broad.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_reward_frontier = exp_reward_frontier.query(\"omega >= 0.85\")\n",
    "direct_frontier = direct_frontier.query(\"param >= 0.85\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the values of the Yelin baseline, run the following\n",
    "\n",
    "```bash\n",
    "cd ${REPO_PATH}/experiments\n",
    "bash scripts/eval_test_scripts/test_yelin_baseline.sh\n",
    "```\n",
    "\n",
    "Or if you are running with replication hyperparameters, use the following\n",
    "\n",
    "```bash\n",
    "cd ${REPO_PATH}/experiments\n",
    "bash scripts/eval_test_scripts/test_yelin_baseline_rep.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yellin Baseline\n",
    "yellin = {\n",
    "    'constrained': [0.10910936310581071,0.33570159857904086],\n",
    "    'unconstrained': [0.054554681552905354,0.965237249429079]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# Our methods\n",
    "plt.scatter(direct_frontier['iat'],\n",
    "           direct_frontier['broad'],\n",
    "           label='Direct', \n",
    "           s=60)\n",
    "\n",
    "plt.scatter(exp_reward_frontier['iat'],\n",
    "           exp_reward_frontier['broad'],\n",
    "           label='Expected reward', marker=\"D\", s=60)\n",
    "\n",
    "plt.scatter(thresholding_frontier['iat_prop'],\n",
    "           thresholding_frontier['broad_prop'],\n",
    "           label='Thresholding', s=60, marker=\"^\")\n",
    "\n",
    "\n",
    "# Clinicians\n",
    "plt.scatter([0.119], [0.336],\n",
    "          label='Doctor', s=180, marker=\"P\")\n",
    "\n",
    "# Constraint Yelin result\n",
    "plt.scatter(yellin['constrained'][0], yellin['constrained'][1],\n",
    "            label='Constrained [31]', s=150, marker=\"s\",\n",
    "           color='c')\n",
    "\n",
    "# Unconstrained Yelin result\n",
    "plt.scatter(yellin['unconstrained'][0], yellin['unconstrained'][1],\n",
    "            label='Unconstrained [31]',\n",
    "            s=250, marker=\"X\", color='m')\n",
    "\n",
    "plt.xlim(0.05, 0.122)\n",
    "\n",
    "plt.xlabel(\"IAT rate\")\n",
    "plt.ylabel(\"2nd-line usage\")\n",
    "\n",
    "plt.legend(fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values_const_2line(frontier, threshold):\n",
    "    iat = frontier.query('broad < @threshold').sort_values(by='broad', ascending=False).iloc[0]['iat']\n",
    "    broad = frontier.query('broad < @threshold').sort_values(by='broad', ascending=False).iloc[0]['broad']  \n",
    "    return {'IAT': iat, '2nd-line Usage': broad}\n",
    "\n",
    "table_2_dict = {\n",
    "    'Doctor': {'IAT': 0.119, '2nd-line Usage': 0.336},\n",
    "    'Thresholding': get_values_const_2line(thresholding_frontier.rename(\n",
    "        columns={'broad_prop': 'broad', 'iat_prop': 'iat'}), 0.336),\n",
    "    'Expected Reward': get_values_const_2line(exp_reward_frontier, 0.336),\n",
    "    'Direct Learning': get_values_const_2line(direct_frontier, 0.336),\n",
    "}\n",
    "\n",
    "print(pd.DataFrame.from_dict(table_2_dict).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values_improve(frontier, threshold):\n",
    "    iat = frontier.query('iat < @threshold').sort_values(by='iat', ascending=False).iloc[0]['iat']\n",
    "    broad = frontier.query('iat < @threshold').sort_values(by='iat', ascending=False).iloc[0]['broad']  \n",
    "    return {'IAT': iat, '2nd-line Usage': broad}\n",
    "\n",
    "table_3_dict = {\n",
    "    'Doctor': {'IAT': 0.119, '2nd-line Usage': 0.336},\n",
    "    'Thresholding': get_values_improve(thresholding_frontier.rename(\n",
    "        columns={'broad_prop': 'broad', 'iat_prop': 'iat'}), 0.119),\n",
    "    'Expected Reward': get_values_improve(exp_reward_frontier, 0.119),\n",
    "    'Direct Learning': get_values_improve(direct_frontier, 0.119),\n",
    "}\n",
    "\n",
    "print(pd.DataFrame.from_dict(table_3_dict).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify file paths here as needed\n",
    "test_results_deferral = pd.read_csv(f\"{EXP_PATH}/direct/direct_learning_defer_interventions_test/frontier_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.scatter(test_results_deferral['defer_rate'],\n",
    "           test_results_deferral['iat_decision'],\n",
    "           label='Algorithm', s=100)\n",
    "\n",
    "plt.scatter(test_results_deferral['defer_rate'],\n",
    "           test_results_deferral['iat_doc'],\n",
    "           label='Doctor', s=150, marker=\"P\")\n",
    "plt.xlabel(\"Deferral rate\")\n",
    "plt.ylabel(\"IAT rate\")\n",
    "plt.title(\"IAT rate comparison: Decision cohort\",\n",
    "         fontsize=22)\n",
    "plt.ylim(0.08, 0.14)\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "plt.scatter(test_results_deferral['defer_rate'],\n",
    "           test_results_deferral['broad_decision'],\n",
    "           label='Algorithm', s=100)\n",
    "\n",
    "plt.scatter(test_results_deferral['defer_rate'],\n",
    "           test_results_deferral['broad_doc'],\n",
    "           label='Doctor', s=150, marker=\"P\")\n",
    "\n",
    "plt.xlabel(\"Deferral rate\")\n",
    "plt.ylabel(\"2nd-line usage rate\")\n",
    "plt.title(\"2nd-line usage comparison: Decision cohort\",\n",
    "         fontsize=22)\n",
    "\n",
    "plt.legend(fontsize=18, bbox_to_anchor=(1.45, .7))\n",
    "plt.ylim(.0, 0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
